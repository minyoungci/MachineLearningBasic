{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f124822a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 평균 값\n",
      "sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "petal length (cm)    3.758000\n",
      "petal width (cm)     1.199333\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 분산 값\n",
      "sepal length (cm)    0.685694\n",
      "sepal width (cm)     0.189979\n",
      "petal length (cm)    3.116278\n",
      "petal width (cm)     0.581006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
    "\n",
    "print('feature 들의 평균 값')\n",
    "print(iris_df.mean())\n",
    "print('\\nfeature 들의 분산 값')\n",
    "print(iris_df.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1e6f0",
   "metadata": {},
   "source": [
    "## StandardScaler \n",
    "\n",
    "StandardScaler는 표준화를 쉽게 지원하기 위한 클래스. 즉, 개별 피처를 평균이 0이고 분산이 1인 값으로 변환함. \n",
    "이렇게 가우시안 정규 분포를 가질 수 있도록 데이터를 변환하는 것은 매우 중요함.\n",
    "서포트 벡터 머신, 선형 회귀, 로지스틱 회귀는 데이터가 가우시안 분포를 가지고 있다고 가정하고 구현했기 때문에 사전에 표준화를 적용하는 것은 예측 성능 향상에 중요한 요소가 됌.\n",
    "\n",
    "#### scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de4589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 들의 평균 값\n",
      "sepal length (cm)   -1.690315e-15\n",
      "sepal width (cm)    -1.842970e-15\n",
      "petal length (cm)   -1.698641e-15\n",
      "petal width (cm)    -1.409243e-15\n",
      "dtype: float64\n",
      "\n",
      "feature 들의 분산 값\n",
      "sepal length (cm)    1.006711\n",
      "sepal width (cm)     1.006711\n",
      "petal length (cm)    1.006711\n",
      "petal width (cm)     1.006711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성 \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# StandardScalrer 로 데이터 세트 변환. fit()과 transform() 호출\n",
    "\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# transform() 시 스케일 변환된 데이터세트가 NumPy ndarray로 반환돼 이를 DataFrame으로 변환.\n",
    "\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "\n",
    "print('feature 들의 평균 값')\n",
    "print(iris_df_scaled.mean())\n",
    "print('\\nfeature 들의 분산 값')\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede8303",
   "metadata": {},
   "source": [
    "## MinMaxScaler\n",
    "\n",
    "MinMaxScaler 은 데이터값을 0~1 사이의 범위 값으로 변환. 데이터 분포가 가우시안 분포가 아닐 경우 min,max scale 적용 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d142dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature들의 최솟값\n",
      "sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n",
      "\n",
      "feautre 들의 최대값\n",
      "sepal length (cm)    1.0\n",
      "sepal width (cm)     1.0\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# MinMaxScaler 객체 생성 \n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# MinMaxScaler로 데이터 세트 변환. fit() transform() 호출 \n",
    "scaler.fit(iris_df)\n",
    "\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# transform()시 스케일 변환된 데이터 세트가 numpy ndarray로 반환돼 이를 df로 변환 \n",
    "\n",
    "iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)\n",
    "\n",
    "print('feature들의 최솟값')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nfeautre 들의 최대값')\n",
    "print(iris_df_scaled.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b78c8",
   "metadata": {},
   "source": [
    "## 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점\n",
    "\n",
    "학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야함.\n",
    "테스트 데이터로 다시 새로운 스케일링 기준 정보를 만들게 되면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 서로 달라지기 때문에 올바른 \n",
    "예측 결과를 도출할 수 xx.\n",
    "\n",
    "반드시 학습 데이터의 스케일링 기준에 따라 테스트 데이터를 transform()해야함. 테스트 데이터에 fit()사용 xx\n",
    "\n",
    "fit_transform() 역시 학습 데이터에는 상관없지만 테스트 데이터에 절대 사용 xx\n",
    "\n",
    "## 요약\n",
    "\n",
    "### 1. 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리\n",
    "### 2. 1이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fill()이 된 Scaler 객체를 이용해 transform() 으로 변환. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
